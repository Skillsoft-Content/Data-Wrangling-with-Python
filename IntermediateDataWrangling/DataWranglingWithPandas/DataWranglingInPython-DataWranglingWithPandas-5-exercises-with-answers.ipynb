{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb227961",
   "metadata": {},
   "source": [
    "## DATAWRANGLINGINPYTHON/DATAWRANGLINGWITHPANDAS/DATAWRANGLINGINPYTHON DATAWRANGLINGWITHPANDAS 5 EXERCISE ANSWERS ##\n",
    "#### Please refer to module 1 of DataWranglingInPython - DataWranglingWithPandas for Tasks 1-4\n",
    "#### Task 1\n",
    "##### Import `pandas` as `pd` and numpy as `np`.\n",
    "##### Import `path` from `Pathlib`.\n",
    "##### Set the data directory using the data_dir variable.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a2d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "# Set 'main_dir' to location of the project folder\n",
    "home_dir = Path(\".\").resolve()\n",
    "main_dir = home_dir.parent.parent\n",
    "print(main_dir)\n",
    "data_dir = str(main_dir) + \"/data\"\n",
    "print(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee956f8f",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Create a Pandas `Series` object from a list of groceries below. Assign it to `shopping_series` variable and print it.\n",
    "- milk\n",
    "- eggs\n",
    "- peppers\n",
    "- cheese\n",
    "- tomatoes\n",
    "##### Print out only the values of the Series.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b736f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shopping_series = pd.Series([\"milk\", \"eggs\", \"peppers\", \"cheese\", \"tomatoes\"])\n",
    "print(shopping_series)\n",
    "print(shopping_series.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372b7cbc",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Create a Pandas Date Series object starting from `January 1st 2019` and ending with `January 1st 2020` separated monthly. Assign the Series to `monthly_series` variable. Print the result.     \n",
    "##### Create another Pandas Date Series object starting from `January 1st 2019` and covering 12 monthly periods. Assign the Series to `monthly_series_by_period` variable. Print the result.\n",
    "##### Are the two Series equivalent?\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f043e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_series = pd.date_range(start = '20190101', end = '20200101', freq = 'M')\n",
    "print(monthly_series)\n",
    "monthly_series_by_period = pd.date_range(start = '20190101', freq = 'M', periods = 12)\n",
    "print(monthly_series_by_period)\n",
    "# Yes, the two series objects are equivalent!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e527124b",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Create a NumPy array `abs_values` with values `[5, 8, 66, 4]` and convert it into a `Pandas Series` object. Assign it to variable `abs_series`.\n",
    "##### Print the result.\n",
    "##### Then print the results of the following operations:\n",
    "- Get the shape of the Series\n",
    "- Find the mean of the Series\n",
    "- Find the median of the Series\n",
    "- Find the standard deviation of the Series\n",
    "- Get unique values in the Series\n",
    "- Get the number of unique values in the Series\n",
    "#### Here are some bonus questions that you could work on later\n",
    "- Get counts of unique values in the Series\n",
    "- Find the position of the minimum value in the Series\n",
    "- Find the position of the maximum value in the Series\n",
    "- Rank items in the Series in descending order\n",
    "- Sort values in the Series\n",
    "- Find sum of the Series\n",
    "- Find cumulative sum of the Series\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc08c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_values = np.array([5, 8, 66, 4])\n",
    "abs_series = pd.Series(abs_values)\n",
    "print(abs_series)\n",
    "print(abs_series.shape)\n",
    "print(abs_series.mean())\n",
    "print(abs_series.median())\n",
    "print(abs_series.std())\n",
    "print(abs_series.unique())\n",
    "print(abs_series.nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af28a43",
   "metadata": {},
   "source": [
    "#### Answers for the bonus questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcf3585",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(abs_series.value_counts())\n",
    "print(abs_series.idxmin()) \n",
    "print(abs_series.idxmax())\n",
    "print(abs_series.rank(ascending = False))\n",
    "print(abs_series.sort_values())\n",
    "print(abs_series.sum())\n",
    "print(abs_series.cumsum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b052c9",
   "metadata": {},
   "source": [
    "#### Please refer to module 2 of DataWranglingInPython - DataWranglingWithPandas for Tasks 5-8\n",
    "#### Task 5\n",
    "##### Create a Pandas Series object from a list of groceries below. Assign it to a variable called `shopping_series` and print it.\n",
    "- milk\n",
    "- eggs\n",
    "- peppers\n",
    "- cheese\n",
    "- tomatoes\n",
    "##### Create a Series object called `prices` that contains the following values: [2.5, 3.99, 1.95, 5.99, 1.29].\n",
    "##### Create a Series object called `num_items` that contains the following values: [1, 1, 2, 1, 3].\n",
    "##### Combine the three `Series` objects described above into a single DataFrame called `shopping_cart` naming the columns as follows:\n",
    "- `item_name`\n",
    "- `item_price`\n",
    "- `item_quantity`\n",
    "##### Print the first few entries (i.e. the `head` of the DataFrame).\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c945a69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shopping_series = pd.Series([\"milk\", \"eggs\", \"peppers\", \"cheese\", \"tomatoes\"])\n",
    "prices = pd.Series([2.5, 3.99, 1.95, 5.99, 1.29])\n",
    "num_items = pd.Series([1, 1, 2, 1, 3])\n",
    "shopping_cart = pd.DataFrame({'item_name': shopping_series, 'item_price': prices, 'item_quantity': num_items})\n",
    "print(shopping_cart.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a479b774",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "##### Check the following characteristics of the `shopping_cart` object:\n",
    "- Type\n",
    "- Shape\n",
    "- Show column names of the DataFrame\n",
    "- Show information of the DataFrame\n",
    "- Describe the DataFrame\n",
    "##### How many columns are described in the last output? Why do you think that's the case?\n",
    "##### What is the 75th percentile of the column `item_price`?\n",
    "##### What is the standard deviation of the column `item_quantity`?\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91c7eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(shopping_cart))\n",
    "print(shopping_cart.shape)\n",
    "print(shopping_cart.columns)\n",
    "print(shopping_cart.info())\n",
    "print(shopping_cart.describe())\n",
    "# There are only 2 columns because the first column is non-numerical.\n",
    "# The 75th percentile of `item_price` is 3.99.\n",
    "# The standard deviation of `item_quantity` is 0.894427.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a54e72",
   "metadata": {},
   "source": [
    "#### Task 7\n",
    "##### Perform the following on the `shopping_cart` DataFrame:\n",
    "- Extract column `item_price`\n",
    "- Extract columns `item_name` and`item_quantity`\n",
    "- Extract the 3rd row\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550feb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shopping_cart['item_price'])\n",
    "print(shopping_cart[['item_name', 'item_quantity']])\n",
    "print(shopping_cart.iloc[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea94cf6",
   "metadata": {},
   "source": [
    "#### Task 8\n",
    "##### Set the index of `shopping_cart` DataFrame to `item_name`. Print the updated DataFrame.\n",
    "##### Print just the index of the `shopping_cart` DataFrame.\n",
    "##### Look up the row of the `shopping_cart` by using the index value 'eggs'.\n",
    "##### Reset index of the `shopping_cart`. Print the updated DataFrame.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac8ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "shopping_cart = shopping_cart.set_index('item_name')\n",
    "print(shopping_cart)\n",
    "print(shopping_cart.index)\n",
    "print(shopping_cart.loc['eggs'])\n",
    "shopping_cart = shopping_cart.reset_index()\n",
    "print(shopping_cart)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eb9e6f",
   "metadata": {},
   "source": [
    "#### Please refer to module 3 of DataWranglingInPython - DataWranglingWithPandas for Tasks 9-12\n",
    "#### Task 9\n",
    "##### Read in 'heart_failure_clinical_records_dataset.csv' into a variable called `ex_df` using Pandas.\n",
    "##### Print the first 20 rows of the DataFrame.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_df = pd.read_csv(str(data_dir)+'/'+ 'heart_failure_clinical_records_dataset.csv')\n",
    "print(ex_df.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b462e",
   "metadata": {},
   "source": [
    "#### Task 10\n",
    "##### Perform the following actions on `ex_df` object:\n",
    "- Check the type of the object\n",
    "- Get the shape of the object and save it to 2 variables and print both: `nrows` and `ncols`\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfb11cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(ex_df))\n",
    "nrows, ncols = ex_df.shape\n",
    "print(nrows)\n",
    "print(ncols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330345c6",
   "metadata": {},
   "source": [
    "#### Task 11\n",
    "##### Use the sample function to:\n",
    "- print 3 random rows in ex_df\n",
    "- print 0.1% random rows in \n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6509f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ex_df.sample(n=3))\n",
    "print(ex_df.sample(frac=0.001))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4f86f5",
   "metadata": {},
   "source": [
    "#### Task 12\n",
    "##### Inspect `ex_df` for the following information:\n",
    "- columns\n",
    "- data types\n",
    "- info\n",
    "- statistical summary (describe)\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b91ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_df.columns\n",
    "ex_df.dtypes\n",
    "ex_df.info()\n",
    "ex_df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3be277",
   "metadata": {},
   "source": [
    "#### Please refer to module 4 of DataWranglingInPython - DataWranglingWithPandas for Tasks 13-16\n",
    "#### Task 13\n",
    "##### Find the number of unique values present in the columns of the DataFrame, store the result in the dictionary format with the name `ex_col_dict` and print the dictionary.\n",
    "##### Identify and pick the column which has the least number of unique values from the dictionary.\n",
    "##### Save the column as `ex_grouping_col` and print it.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad1aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_col_dict = ex_df.nunique().to_dict()\n",
    "print(ex_col_dict)\n",
    "ex_grouping_col = min(ex_col_dict, key=ex_col_dict.get)\n",
    "ex_grouping_col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dc070f",
   "metadata": {},
   "source": [
    "#### Task 14\n",
    "##### Group by the column `ex_grouping_col` and assign it to a DataFrame named `ex_grouped_df`.\n",
    "##### Use the summary functions to get the count of ID variable for all the variables you just grouped in `ex_grouped_df`.\n",
    "##### Assign the output to a DataFrame called `ex_df_ID` and print that DataFrame.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe4fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_grouped_df = ex_df.groupby(ex_grouping_col)\n",
    "ex_df_ID = ex_grouped_df.count()[['id']]\n",
    "print(ex_df_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee50adb",
   "metadata": {},
   "source": [
    "#### Task 15\n",
    "##### Sort the values by ID.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0fc397",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_df_ID.sort_values(['id'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b65bc88",
   "metadata": {},
   "source": [
    "#### Task 16\n",
    "##### In `ex_df_ID`, create a new Boolean column (containing values True or False) called `over10000_ID` .\n",
    "##### This indicates whether a row has a value for `ID` that is higher than 10000.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b5bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "over10000_ID = ex_df_ID[['id']] > 10000\n",
    "# Add the new column.\n",
    "ex_df_ID['over10000_ID'] = over10000_ID\n",
    "print(ex_df_ID.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5bff46",
   "metadata": {},
   "source": [
    "#### Please refer to module 5 of DataWranglingInPython - DataWranglingWithPandas for Tasks 17-20\n",
    "#### Task 17\n",
    "##### Subset the DataFrame to have the following variables: ['creatinine_phosphokinase', 'age', 'serum_sodium', 'platelets', 'time', 'serum_creatinine', 'ejection_fraction', 'sex', 'death_event', 'diabetes', 'smoking', 'anaemia', 'high_blood_pressure']. Print head of `ex_df_subset`.\n",
    "##### Check if there are any NAs in the DataFrame and fill with mean if NAs are present.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb907cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_df_subset = ex_df[['creatinine_phosphokinase', 'age', 'serum_sodium', 'platelets', 'time', 'serum_creatinine', 'ejection_fraction', 'sex', 'death_event', 'diabetes', 'smoking', 'anaemia', 'high_blood_pressure']]\n",
    "print(ex_df_subset.head())\n",
    "# Set the DataFrame equal to the imputed dataset.\n",
    "ex_df_subset[['creatinine_phosphokinase', 'age', 'serum_sodium', 'platelets', 'time', 'serum_creatinine', 'ejection_fraction']] = ex_df_subset[['creatinine_phosphokinase', 'age', 'serum_sodium', 'platelets', 'time', 'serum_creatinine', 'ejection_fraction']].fillna(ex_df_subset.mean())\n",
    "# Check how many values are null in the DataFrame.\n",
    "print(ex_df_subset.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8638483f",
   "metadata": {},
   "source": [
    "#### Task 18\n",
    "##### Group `ex_df` data by the variable `ex_grouping_col`. Save as `ex_grouped`.\n",
    "##### Then group and summarize the numeric variables: ['creatinine_phosphokinase', 'age', 'serum_sodium', 'platelets', 'time', 'serum_creatinine', 'ejection_fraction'] using their means.\n",
    "##### Save as `ex_grouped_mean` and print.\n",
    "##### Reset its index and print the result.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7194437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by `per_capita_income` variable.\n",
    "ex_grouped = ex_df.groupby(ex_grouping_col)\n",
    "# Compute mean on the listed variables using the grouped data.\n",
    "ex_df_grouped_mean = ex_grouped.mean()[['creatinine_phosphokinase', 'age', 'serum_sodium', 'platelets', 'time', 'serum_creatinine', 'ejection_fraction']]\n",
    "print(ex_df_grouped_mean)\n",
    "# Reset index of the dataset.\n",
    "ex_df_grouped_mean = ex_df_grouped_mean.reset_index()\n",
    "print(ex_df_grouped_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6de4d8",
   "metadata": {},
   "source": [
    "#### Task 19\n",
    "##### Notice the format of `ex_grouped_mean`. We wish to convert it from wide to long format.\n",
    "##### Use the `pd.melt()` function and convert it to long format. Save as `ex_grouped_mean_long` and print the result.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8febd7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the wide data into long.\n",
    "ex_grouped_mean_long = pd.melt(ex_df_grouped_mean,       #<- wide dataset\n",
    "                               id_vars = [ex_grouping_col],  #<- identifying variable\n",
    "                               var_name = 'metric',      #<- contains col names of wide data\n",
    "                               value_name = 'mean')      #<- contains values from above columns\n",
    "print(ex_grouped_mean_long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d5236f",
   "metadata": {},
   "source": [
    "#### Task 20\n",
    "##### Now use the `pd.pivot()` function to convert `ex_grouped_mean_long` to wide format.\n",
    "##### Save as `ex_grouped_mean_wide` and print.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52c9f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the long data into wide.\n",
    "ex_grouped_mean_wide = ex_grouped_mean_long.pivot(\n",
    "                                      index = [ex_grouping_col],     #<- identifying variable\n",
    "                                      columns = 'metric', #<- contains col names of wide data\n",
    "                                      values = 'mean')    #<- contains values from above columns\n",
    "print(ex_grouped_mean_wide)\n"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
